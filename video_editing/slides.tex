\documentclass[aspectratio=169, 20pt]{beamer}
\input{preamble}

\title{Language Bootstrapping: Learning Word Meanings From Perception-Action Association}
\author{Giampiero Salvi, Luis Montesano,\\Alexandre Bernardino, José Santos-Victor}
\date{}

\begin{document}
\begin{frame}
  \maketitle
\end{frame}

\begin{frame}{LEARNING AFFORDANCES}
\large After gaining some knowledge about individual object and effect properties, the robot learns to associate object features to the consequences of its actions. It thus creates a model of object affordances.
\end{frame}

\begin{frame}{\large LEARNED AFFORDANCE NETWORK}
  \begin{center}
    \resizebox{!}{0.65\textheight}{
    \tikzstyle{affnode} = [ellipse, draw=gray!60, text=black, fill=gray!60, ultra thick, font=\bf]%fill=gray!20, thick]%node distance=1cm, text width=6em, text centered, minimum height=4em, thick]
    \tikzstyle{affarrow} = [->, ultra thick, >=stealth']%, shorten >=2pt]%, shorten >=2pt]
    \centering
    \begin{tikzpicture}[node distance=1cm and 2cm]
      % single nodes
      \node[affnode] (action) {Action};
      \node[affnode, right= of action] (size) {Size};
      \node[affnode, right= of size] (shape) {Shape};
      \node[affnode, right= of shape] (color) {Color};
      \node[affnode, below= of action] (handvel) [xshift=3cm] {Hand Vel};
      \node[affnode, right= of handvel] (objvel) {Obj Vel};
      \node[affnode, below= of handvel] (objhandvel) [xshift=3cm] {Obj Hand Vel};
      \node[affnode, below= of objhandvel] (contact) {Contact};
      % arrows
      \draw[affarrow] (action) -- (handvel);
      \draw[affarrow] (action) -- (objvel);
      \draw[affarrow] (action) to [out=260,in=170] (objhandvel);
      \draw[affarrow] (action) to [out=230,in=160] (contact);
      \draw[affarrow] (size) -- (objvel);
      \draw[affarrow] (shape) -- (objvel);
      \draw[affarrow] (shape) to [out=330,in=20] (contact);
      \draw[affarrow] (objvel) -- (objhandvel);
    \end{tikzpicture}
    }
  \end{center}
\end{frame}

\begin{frame}{LEARNING WORD-TO-MEANING ASSOCIATIONS}
\large While listening to spoken descriptions of the task provided by humans, the robot can learn to associate words to perceptions and actions by exploiting their co-occurrence.
\end{frame}

\begin{frame}
  \large After a training stage, the robot has learned the following word-to-meaning associations.

  \vspace{1cm}
  White nodes: words
  
  Gray nodes: meanings
\end{frame}

\begin{frame}{OBJECT PROPERTIES}
  \begin{center}
    \begin{tikzpicture}[every node/.style={inner sep=0,outer sep=0}]
    \node[inner sep=0] (color) at (0,0) {\includegraphics[height=0.5\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_one_parents_Color.pdf}};
    \node[inner sep=0] (size) at (6,0) {\includegraphics[height=0.5\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_one_parents_Size.pdf}};
    \node[inner sep=0] (shape) at (2.5,-3) {\includegraphics[height=0.5\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_one_parents_Shape.pdf}};
    \begin{scope}[on background layer]
      \node[fill=white, inner sep=-5mm, fit=(color) (size) (shape)]{};
    \end{scope}
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{EFFECTS OF ACTIONS}
  \resizebox{\textwidth}{!}{
    %\fbox{
    \begin{tikzpicture}[every node/.style={inner sep=-9mm,outer sep=-9mm}]
      \node (one) at (0,0) {\includegraphics[height=0.45\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_one_parents_ObjVel.pdf}};
      \node (two) at (8,0) {\includegraphics[height=0.45\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_two_parents_HandVelContact.pdf}};
      \node (three) at (0,-3) {\includegraphics[height=0.45\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_three_parents_ObjVelHandVelContact.pdf}};
      \node (four) at (8,-3) {\includegraphics[height=0.45\textheight, trim=0 0 0 0]{/home/giampi/Documents/vislab/usr/gsalvi/ieeesmcb2010_paper/figures/netobj_lab_two_parents_ActionShape.pdf}};
      \begin{scope}[on background layer]
        \node[fill=white, inner sep=12mm, fit=(one) (two) (three) (four)]{};
      \end{scope}
    \end{tikzpicture}
    %}
  }
\end{frame}

% here should add the word-affordance network details

\begin{frame}{\large USING WORD-TO-MEANING ASSOCIATIONS}
  The robot has learned to interpret verbal instructions that are possibly ambiguous or incomplete.
It uses the environment configuration together with the learned models to choose the appropriate action and object to operate.
\end{frame}

\begin{frame}{ACKNOWLEDGMENTS}
  FCT grant number: SFRH/BPD/35050/2007

  \vspace{8mm}
  Vetenskapsrådet grant number: 90459901

  \vspace{8mm}
  Narrating voice by Google TTS
\end{frame}

\begin{frame}{\large MORE INFORMATION}
  Paper DOI: 10.1109/TSMCB.2011.2172420

  \vspace{8mm}
  Open access version: \url{https://arxiv.org/abs/1711.09714}

  \vspace{8mm}
  Code: \url{https://github.com/giampierosalvi/AffordancesAndSpeech}
\end{frame}



\end{document}
